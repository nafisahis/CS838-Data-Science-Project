{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# reading positive and negative examples\n",
    "pos_file = \"Downloads/pos_feat_vec.csv\"\n",
    "neg_file = \"Downloads/neg_feat_vec.csv\"\n",
    "\n",
    "pos = pd.read_csv(pos_file, header = 0)\n",
    "neg = pd.read_csv(neg_file, header = 0)\n",
    "\n",
    "# combine positive and negative training examples into a full training set\n",
    "data = pd.concat([pos, neg])\n",
    "\n",
    "# shuffling the data to avoid bias\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = shuffle(data, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# optimizing the classifiers\n",
    "#size_train = round(0.7 * len(data))\n",
    "\n",
    "#train = data.iloc[0:size_train + 1, :]\n",
    "#tune = data.iloc[size_train + 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "names = data.ix[:,0]\n",
    "#name = train.ix[:,0]\n",
    "labels = data.ix[:, len(data.columns) - 1]\n",
    "#labels = train.ix[:, len(data.columns) - 1]\n",
    "features = data.ix[:, 1: len(data.columns) - 1]\n",
    "#features = train.ix[:, 1: len(data.columns) - 1]\n",
    "\n",
    "# convert pandas dataframes into numpy arrays (compatible with scikit-learn)\n",
    "features = features.as_matrix()\n",
    "labels = labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8227997643035847\n",
      "Precision:  0.9175089411333719\n",
      "Recall:  0.5847183668759286\n"
     ]
    }
   ],
   "source": [
    "# training decision trees and performing cross-validation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fold = 10\n",
    "threshold = 0.85\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion = 'entropy', max_depth = 10)\n",
    "kf = KFold(n_splits = fold, shuffle = True)\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    features_train, features_test = features[train_index], features[test_index]\n",
    "    labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    dt.fit(features_train, labels_train)\n",
    "    result = dt.predict_proba(features_test)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    for k in range(0, len(result)):\n",
    "        if labels_test[k] == 1:\n",
    "            if result[k][1] > threshold:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if result[k][1] > threshold:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    accuracy.append((tp + tn) / len(result))\n",
    "    precision.append(tp / (tp + fp))\n",
    "    recall.append(tp / (tp + fn))\n",
    "\n",
    "print('Accuracy: ', sum(accuracy) / fold)\n",
    "print('Precision: ', sum(precision) / fold)\n",
    "print('Recall: ', sum(recall) / fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8311853404557441\n",
      "Precision:  0.9598585275461622\n",
      "Recall:  0.5787101394836041\n"
     ]
    }
   ],
   "source": [
    "# training random forests and performing cross-validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "fold = 10\n",
    "threshold = 0.85\n",
    "\n",
    "rf = RandomForestClassifier(criterion = 'gini', n_estimators = 10, max_depth = 15)\n",
    "kf = KFold(n_splits = fold, shuffle = True)\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    features_train, features_test = features[train_index], features[test_index]\n",
    "    labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    rf.fit(features_train, labels_train)\n",
    "    result = rf.predict_proba(features_test)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    for k in range(0, len(result)):\n",
    "        if labels_test[k] == 1:\n",
    "            if result[k][1] > threshold:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if result[k][1] > threshold:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    accuracy.append((tp + tn) / len(result))\n",
    "    precision.append(tp / (tp + fp))\n",
    "    recall.append(tp / (tp + fn))\n",
    "\n",
    "print('Accuracy: ', sum(accuracy) / fold)\n",
    "print('Precision: ', sum(precision) / fold)\n",
    "print('Recall: ', sum(recall) / fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.875603247587\n",
      "Precision:  0.84368853349\n",
      "Recall:  0.82533225393\n"
     ]
    }
   ],
   "source": [
    "# training SVMs and performing cross-validation\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "s = svm.SVC()\n",
    "fold = 10\n",
    "\n",
    "s.fit(features, labels)\n",
    "\n",
    "accuracy = cross_val_score(s, features, labels, cv = fold)\n",
    "precision = cross_val_score(s, features, labels, cv = fold, scoring = 'precision')\n",
    "recall = cross_val_score(s, features, labels, cv = fold, scoring = 'recall')\n",
    "\n",
    "print('Accuracy: ', accuracy.sum() / fold)\n",
    "print('Precision: ', precision.sum() / fold)\n",
    "print('Recall: ', recall.sum() / fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8432959342368361\n",
      "Precision:  0.7967262460717449\n",
      "Recall:  0.787193239979846\n"
     ]
    }
   ],
   "source": [
    "# training linear regressors and performing cross-validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "fold = 10\n",
    "threshold = 0.5\n",
    "rg = LinearRegression()\n",
    "kf = KFold(n_splits = fold, shuffle = True)\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in kf.split(features):\n",
    "    features_train, features_test = features[train_index], features[test_index]\n",
    "    labels_train, labels_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    rg.fit(features_train, labels_train)\n",
    "    result = rg.predict(features_test)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "\n",
    "    for k in range(0, len(result)):\n",
    "        if labels_test[k] == 1:\n",
    "            if result[k] > threshold:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if result[k] > threshold:\n",
    "                fp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "\n",
    "    accuracy.append((tp + tn) / len(result))\n",
    "    precision.append(tp / (tp + fp))\n",
    "    recall.append(tp / (tp + fn))\n",
    "\n",
    "print('Accuracy: ', sum(accuracy) / fold)\n",
    "print('Precision: ', sum(precision) / fold)\n",
    "print('Recall: ', sum(recall) / fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.876534441622\n",
      "Precision:  0.861631450873\n",
      "Recall:  0.804422181745\n"
     ]
    }
   ],
   "source": [
    "# training logistic regressors and performing cross-validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "lr = LogisticRegression()\n",
    "fold = 10\n",
    "\n",
    "lr.fit(features, labels)\n",
    "\n",
    "accuracy = cross_val_score(dt, features, labels, cv = fold)\n",
    "precision = cross_val_score(dt, features, labels, cv = fold, scoring='precision')\n",
    "recall = cross_val_score(dt, features, labels, cv = fold, scoring='recall')\n",
    "\n",
    "print('Accuracy: ', accuracy.sum() / fold)\n",
    "print('Precision: ', precision.sum() / fold)\n",
    "print('Recall: ', recall.sum() / fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reading test examples\n",
    "test_file = 'Downloads/test_feat_vec.csv'\n",
    "test = pd.read_csv(test_file, header = 0)\n",
    "\n",
    "test_names = test.ix[:,0]\n",
    "#test_names = tune.ix[:,0]\n",
    "test_positions = test.ix[:, 1: 3]\n",
    "#test_positions = tune.ix[:, 1: 3]\n",
    "test_features = test.ix[:, 3:]\n",
    "#test_featuress = tune.ix[:, 3:]\n",
    "\n",
    "test_names = test_names.as_matrix()\n",
    "#test_names = test_names.as_matrix()\n",
    "test_positions = test_positions.as_matrix()\n",
    "#test_positions = test_positions.as_matrix()\n",
    "test_features = test_features.as_matrix()\n",
    "#test_features = test_features.as_matrix()\n",
    "\n",
    "# making prediction on test examples\n",
    "rf_result = rf.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile a list of product names in test documents (for computing precision and recall)\n",
    "product_list = 'Downloads/test_product_list.txt'\n",
    "\n",
    "with open(product_list, 'r') as f:\n",
    "    ref = f.readlines()\n",
    "\n",
    "ref_names = []\n",
    "ref_positions = []\n",
    "\n",
    "for i, product in enumerate(ref):\n",
    "    ref[i] = product.split()\n",
    "    product_name = ' '.join(word for word in ref[i][:-2])\n",
    "    start_pos = int(ref[i][-2])\n",
    "    end_pos = int(ref[i][-1])\n",
    "\n",
    "    ref_names.append(product_name)\n",
    "    ref_positions.append([start_pos, end_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "706 550 8073 190\n",
      "Total number of test examples:  9519\n",
      "Accuracy:  0.9222607416745456\n",
      "Precision:  0.5621019108280255\n",
      "Recall:  0.566158781074579\n"
     ]
    }
   ],
   "source": [
    "# computing precision and recall of the classifier on the test set\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "#threshold = 0.85\n",
    "\n",
    "for i, test_name in enumerate(test_names):   \n",
    "    found = False\n",
    "    \n",
    "    for j, ref_name in enumerate(ref_names):\n",
    "        if test_name == ref_name and test_positions[i][0] == ref_positions[j][0] and test_positions[i][1] == ref_positions[j][1]:\n",
    "            found = True\n",
    "            \n",
    "            #if rf_result[i][0] > threshold:\n",
    "            if s_result[i] == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "            break\n",
    "    \n",
    "    if not found:\n",
    "        #if rf_result[i][1] > threshold:\n",
    "        if s_result[i] == 1:\n",
    "            fp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "\n",
    "print(tp, fp, tn, fn)\n",
    "print('Total number of test examples: ', tp + fp + tn + fn)\n",
    "\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / len(ref_names)\n",
    "\n",
    "print('Accuracy: ', accuracy)\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out all predictions\n",
    "with open('Downloads/test_predict.txt', 'w') as f:\n",
    "    for i, test_name in enumerate(test_names):\n",
    "        print(test_name, rf_result[i][0], rf_result[i][1], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out examples predicted as true (as reference for improving precision)\n",
    "predicted_true = []\n",
    "\n",
    "with open('Downloads/test_predict_positives.txt', 'w') as f:\n",
    "    for i, test_name in enumerate(test_names):\n",
    "        if rf_result[i][1] > threshold:\n",
    "            predicted_true.append([test_name, rf_result[i][0], rf_result[i][1]])\n",
    "            print(test_name, rf_result[i][0], rf_result[i][1], file = f)\n",
    "\n",
    "predicted_true = sorted(predicted_true, key=lambda prediction: prediction[2], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Downloads/sorted_test_predict_positives.txt', 'w') as f:\n",
    "    for tuple in predicted_true:\n",
    "        found = False\n",
    "        \n",
    "        for j, ref_name in enumerate(ref_names):\n",
    "            if tuple[0] == ref_name:\n",
    "                print('true example: ', tuple[0], tuple[1], tuple[2], file = f)\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print('false example: ', tuple[0], tuple[1], tuple[2], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print out examples predicted as false (as reference for improving recall)\n",
    "predicted_false = []\n",
    "\n",
    "with open('Downloads/test_predict_negatives.txt', 'w') as f:\n",
    "    for i, test_name in enumerate(test_names):\n",
    "        if rf_result[i][1] < threshold:\n",
    "            predicted_false.append([test_name, rf_result[i][0], rf_result[i][1]])\n",
    "            print(test_name, rf_result[i][0], rf_result[i][1], file = f)\n",
    "\n",
    "predicted_false = sorted(predicted_false, key=lambda prediction: prediction[2], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('Downloads/sorted_test_predict_negatives_>0.7.txt', 'w') as f:\n",
    "    for tuple in predicted_false:\n",
    "        if tuple[2] < 0.7:\n",
    "            break\n",
    "        \n",
    "        found = False\n",
    "        \n",
    "        for j, ref_name in enumerate(ref_names):\n",
    "            if tuple[0] == ref_name:\n",
    "                print('true example: ', tuple[0], tuple[1], tuple[2], file = f)\n",
    "                found = True\n",
    "                break\n",
    "        \n",
    "        if not found:\n",
    "            print('false example: ', tuple[0], tuple[1], tuple[2], file = f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
